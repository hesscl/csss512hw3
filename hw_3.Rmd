---
title: "CSSS 512 HW 3"
author: "Chris Hess"
date: "May 10, 2018"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(forecast)
library(plm)
library(nlme)
library(simcf)

#key theoretical perspectives:

#1. economic performance
#2. spillovers/coattails
#3. trends

#M1 - control for midterm effects, presidential and gubernatorial unemployment 
#     effects, and presidential and gubernatorial coattails

#M2 - control for all M1 variables and region specific trends
```

## Problem 1

### Part a

```{r prob 1a}
#process data
house <- read_csv("statehouse.csv") %>%
  filter(GovCycle == 1, #gov cycle needs to be 2010, 2014, 2018...
         HouseTerm == 2) %>% #2 yr house term only
  arrange(State, Year) %>% #arrange by State, then year within States
  group_by(State) %>% #group rows by State for lagging
  mutate(DemHouseShareL1 = lag(DemHouseShare, 1), #use dplyr::lag with properly ordered tbl
         DemHouseShareL2 = lag(DemHouseShare, 2),
         DemHouseShareL3 = lag(DemHouseShare, 3),
         DemHouseShareL4 = lag(DemHouseShare, 4))

#double check work using dplyr::lag()
#house %>% 
#  select(State, Year, DemHouseShare, DemHouseShareL1, DemHouseShareL2, DemHouseShareL3, DemHouseShareL4) %>% 
#  View()

#28 states
length(unique(house$State))

#20 periods
length(unique(house$Year))
```

### Part b

Looks like AR(1) processes in most states. Some of the observed time-series look non-stationary given no reversion to a mean over the full length of periods. The nonstationary conclusion is also supported by a fair number `adf.test` and `PP.test` p-values falling outside of the conventional p<.05 threshold. States with clear change over the period show decreashing democratic vote shares over the full set of periods, though there are states like New York which oppose this direction. The IPS tests suggest that we could consider the panel as stationary with individual intercepts and trends.

```{r prob 1b}
#make some file structure for output
if(!dir.exists("output")){
  dir.create("output")
  dir.create("output/ts")
  dir.create("output/acf")
  dir.create("output/pacf")
}

#empty obj for unit root test results
unitroot.tbl <- NULL

#iterate through states
for(state in unique(house$State)){
  
  #pull the state's ts
  ts <- house %>%
    filter(State == state) %>%
    pull(DemHouseShare)
  
  #make it ts class
  ts <- ts(ts, start = 1978, end = 2016, frequency = 1/2)
  
  #plot the ts
  TS <- autoplot(ts) +
    scale_y_continuous(labels = scales::percent) +
    labs(title = paste0("Democratic House Share for ", house$Statename[house$State == state])) +
    theme_minimal()
  
  #plot the ACF
  ACF <- ggAcf(ts) +
    labs(title = paste0("ACF for ", house$Statename[house$State == state])) +
    theme_minimal()
  
  #plot the PACF
  PACF <- ggPacf(ts) +
    labs(title = paste0("PACF for ", house$Statename[house$State == state])) +
    theme_minimal()
  
  #rum ADF test, parse results to row
  ADF <- tseries::adf.test(ts)
  ADF.row <- data.frame(state = state, 
                        test = "ADF", 
                        statistic = ADF[['statistic']], 
                        parameter = ADF[['parameter']], 
                        p.value = ADF[['p.value']],
                        stringsAsFactors = F)
  
  #run PP test, parse results to row
  PP <- PP.test(ts)
  PP.row <- data.frame(state = state, 
                       test = "PP", 
                       statistic = PP[['statistic']], 
                       parameter = PP[['parameter']],
                       p.value = PP[['p.value']],
                       stringsAsFactors = F)

  #write graphics to pdf
  ggsave(TS, filename = paste0("./output/ts/", state, "_ts.pdf"),
         width = 8, height = 6, dpi = 300)
  ggsave(ACF, filename = paste0("./output/acf/", state, "_acf.pdf"),
         width = 8, height = 6, dpi = 300)
  ggsave(PACF, filename = paste0("./output/pacf/", state, "_pacf.pdf"),
         width = 8, height = 6, dpi = 300)
  
  #add row to unit root tbl
  unitroot.tbl <- bind_rows(unitroot.tbl, ADF.row, PP.row)
}

#panel unit root test
ts <- with(house, data.frame(split(DemHouseShare, as.character(State))))
purtest(ts, pmax = 4, exo = "intercept", house_p, test = "ips")
purtest(ts, pmax = 4, exo = "trend", house_p, test = "ips")

#write unit root tbl to storage
write_csv(unitroot.tbl, "output/unitroot_tests.csv")

#plot unit root p.val dist
ggplot(unitroot.tbl, aes(x = p.value, fill = test)) + 
  facet_wrap(~ test) +
  geom_histogram(binwidth = .05) +
  theme_minimal() +
  scale_fill_brewer(palette = "Set1")
```

### Part c

```{r prob 1.c}
house <- groupedData(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails | State, data = house)

#no dynamics, just covariates + state random effect
re_m1_a <- lme(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails,
            random = ~ 1,
            data = house)
summary(re_m1_a)

#AR(1)
re_m1_b <- lme(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails,
            random = ~ 1,
            correlation = corARMA(p = 1, q = 0),
            data = house)
summary(re_m1_b)

#MA(1)
re_m1_c <- lme(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails,
            random = ~ 1,
            correlation = corARMA(p = 0, q = 1),
            data = house)
summary(re_m1_c)

#ARMA(1,1)
re_m1_d <- lme(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails,
            random = ~ 1,
            correlation = corARMA(p = 1, q = 1),
            data = house)
summary(re_m1_d)

#AR(1) appears to fit best. The MA(1) contribution is small, and higher-order 
#autoregression also does not seem to be present
```

 
### Part d 
 
```{r prob 1.d}
#M1 + region specific trends
house <- groupedData(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year + (Year * Midwest) + (Year * South) + (Year * West) | State, data = house)

#no dynamics, just covariates + state random effect
re_m2_a <- lme(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year * Midwest + Year * South + Year * West,
            random = ~ 1,
            data = house)
summary(re_m2_a)

#AR(1)
re_m2_b <- lme(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year * Midwest + Year * South + Year * West,
            random = ~ 1,
            correlation = corARMA(p = 1, q = 0),
            data = house)
summary(re_m2_b)

#MA(1)
re_m2_c <- lme(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year * Midwest + Year * South + Year * West,
            random = ~ 1,
            correlation = corARMA(p = 0, q = 1),
            data = house)
summary(re_m2_c)

#ARMA(1,1)
re_m2_d <- lme(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year * Midwest + Year * South + Year * West,
            random = ~ 1,
            correlation = corARMA(p = 1, q = 1),
            data = house)
summary(re_m2_d)

#model b (AR(1)) is best
```

 
### Part e

```{r prob 1.e}
house_p <- pdata.frame(house, index = "State")

fe_m1_a <- plm(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails,  data= house_p, model = "within", effect = "individual")
summary(fe_m1_a)

fe_m1_b <- plm(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + DemHouseShareL1, data= house_p, model = "within", effect = "individual")
summary(fe_m1_b)

fe_m1_c <- plm(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + DemHouseShareL1 + DemHouseShareL2, data= house_p, model = "within", effect = "individual")
summary(fe_m1_c)

fe_m1_d <- plm(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + DemHouseShareL1 + DemHouseShareL2 + DemHouseShareL3, data= house_p, model = "within", effect = "individual")
summary(fe_m1_d)

#model b (AR(1)) is best
```

### Part f

```{r prob 1.f}
#M1 + region specific trends
fe_m2_a <- plm(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year * South + Year * Midwest + Year * West, data= house_p, model = "within", effect = "individual")
summary(fe_m2_a)

fe_m2_b <- plm(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year * South + Year * Midwest + Year * West + DemHouseShareL1, data= house_p, model = "within", effect = "individual")
summary(fe_m2_b)

fe_m2_c <- plm(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year * South + Year * Midwest + Year * West + DemHouseShareL1 + DemHouseShareL2, data= house_p, model = "within", effect = "individual")
summary(fe_m2_c)

fe_m2_d <- plm(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year * South + Year * Midwest + Year * West + DemHouseShareL1 + DemHouseShareL2 + DemHouseShareL3, data= house_p, model = "within", effect = "individual")
summary(fe_m2_d)

#model b (AR(1)) is best
```

### Part g

```{r prob 1.g}
#Using each of four “best” models, forecast what will happen to
#the size of the Democratic majority in the average state in the 2019 and 2021
#sessions for the following single scenario. Assume the Democrats resume this
#state’s governorship in 2019 and the presidency in 2021, and compute appropriate
#counterfactual values of PartisanMidterm, PresCoattails, GovCoattails. Assume
#unemployment falls to 3.6% for both elections and construct PresUnem and
#GovUnem accordingly. Set all trend variables at the average value they will take
#across regions in 2019 and 2021, respectively. Make appropriate assumptions for
#the prior value(s) of the outcome variable (e.g., the average Democratic House
#share in 2017).

#For each model, report or graph the predicted Democratic majority and its 95%
#confidence (or predictive) interval for the 2019 and 2021 sessions. Describe the
#substantive impact of your forecast results in as much detail as you feel comfortable,
#as well as how much confidence we should have in the forecasts. Be sure to
#consider the scale of the outcome variable in assessing what counts as a substantively
#large or small change.

#NB: As a check on your work, report the table of counterfactual covariate values
#you used to make your forecasts. Be very careful when constructing these values
#to capture to logic of the covariates; each one is tricky in its own way. To carry
#out the forecasts, use the simcf library’s ldvsimev(), pay close attention to the
#example code, and think through all modifications you need to make.

house <- house %>%
  select(State, DemHouseShare, PartisanMidterm, PresUnem, GovUnem, PresCoattails,
         GovCoattails, Year, West, South, Midwest, DemHouseShareL1) %>%
  na.omit() %>%
  arrange(State, Year) %>%
  group_by(State) %>%
  mutate(YearXMidwest = Year* Midwest,
         YearXWest = Year * West,
         YearXSouth = Year * South)
house_p <- pdata.frame(house, index = "State")

#run candidate models
fe_m1_b <- plm(DemHouseShare ~ 1 + PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + DemHouseShareL1, data= house_p, model = "within", effect = "individual")
re_m1_b <- plm(DemHouseShare ~ 1 + PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + DemHouseShareL1, data= house_p, model = "random", effect = "individual")
fe_m2_b <- plm(DemHouseShare ~ 1 + PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year + YearXMidwest + YearXWest + YearXSouth + DemHouseShareL1, data= house_p, model = "within", effect = "individual")
re_m2_b <- plm(DemHouseShare ~ 1 + PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year + Midwest + West + South + YearXMidwest + YearXWest + YearXSouth + DemHouseShareL1, data= house_p, model = "random", effect = "individual")

# Extract model results from FE M1
pe.fe1_b <- coef(fe_m1_b)                                 # Point estimates of parameters
vc.fe1_b <- vcov(fe_m1_b)                                   # Var-cov matrix of point estimates
se.fe1_b <- sqrt(diag(vc.fe1_b))                            # std erros of point estimates
tstat.fe1_b <- abs(pe.fe1_b/se.fe1_b)                        # t-statistics
df.fe1_b <- rep(fe_m1_b$df.residual, length(tstat.fe1_b))  # residual degrees of freedom
pval.fe1_b <- 2*pt(tstat.fe1_b, df.fe1_b, lower.tail=FALSE)  # p-values
fe.fe1_b <- fixef(fe_m1_b)                                # the (removed) fixed effects by group 
resid.fe1_b <- resid(fe_m1_b)       

# Extract model results from FE M2
pe.fe2_b <- coef(fe_m2_b)                                 # Point estimates of parameters
vc.fe2_b <- vcov(fe_m2_b)                                   # Var-cov matrix of point estimates
se.fe2_b <- sqrt(diag(vc.fe2_b))                            # std erros of point estimates
tstat.fe2_b <- abs(pe.fe2_b/se.fe2_b)                        # t-statistics
df.fe2_b <- rep(fe_m2_b$df.residual, length(tstat.fe2_b))  # residual degrees of freedom
pval.fe2_b <- 2*pt(tstat.fe2_b, df.fe2_b, lower.tail=FALSE)  # p-values
fe.fe2_b <- fixef(fe_m2_b)                                # the (removed) fixed effects by group 
resid.fe2_b <- resid(fe_m2_b)                             # Residuals

# Extract model results from RE M1
pe.re1_b <- coef(re_m1_b)                                 # Point estimates of parameters
vc.re1_b <- vcov(re_m1_b)                                   # Var-cov matrix of point estimates
se.re1_b <- sqrt(diag(vc.re1_b))                            # std erros of point estimates
tstat.re1_b <- abs(pe.re1_b/se.re1_b)                        # t-statistics
df.re1_b <- rep(re_m1_b$df.residual, length(tstat.re1_b))  # residual degrees of freedom
pval.re1_b <- 2*pt(tstat.re1_b, df.re1_b, lower.tail=FALSE)  # p-values
re.re1_b <- random.effects(re_m1_b)                                # the (removed) fixed efrects by group 
resid.re1_b <- resid(re_m1_b) 

# Extract model results from RE M2
pe.re2_b <- coef(re_m2_b)                                 # Point estimates of parameters
vc.re2_b <- vcov(re_m2_b)                                   # Var-cov matrix of point estimates
se.re2_b <- sqrt(diag(vc.re2_b))                            # std erros of point estimates
tstat.re2_b <- abs(pe.re2_b/se.re2_b)                        # t-statistics
df.re2_b <- rep(re_m2_b$df.residual, length(tstat.re2_b))  # residual degrees of freedom
pval.re2_b <- 2*pt(tstat.re2_b, df.re2_b, lower.tail=FALSE)  # p-values
re.re2_b <- random.effects(re_m2_b)                                # the (removed) fixed efrects by group 
resid.re2_b <- resid(re_m2_b)                      # Residuals

#set number of simulates
sims <- 10000

# Interpret FE M1
simparam_fe1 <- MASS::mvrnorm(sims,pe.fe1_b,vc.fe1_b)
# Pull off the simulated lag coefficient
simphi_fe1 <- simparam_fe1[,ncol(simparam_fe1)]
# Put together the "constant" term (avg of the FEs, or a specific FE if you like)
# with the rest of the regressors
simbetas_fe1 <- cbind(rep(mean(fe.fe1_b), sims), simparam_fe1[,1:ncol(simparam_fe1)-1])

# Interpret FE M2
simparam_fe2 <- MASS::mvrnorm(sims,pe.fe2_b,vc.fe2_b)
# Pull off the simulated lag coefficient
simphi_fe2 <- simparam_fe2[,ncol(simparam_fe2)]
# Put together the "constant" term (avg of the FEs, or a specific FE if you like)
# with the rest of the regressors
simbetas_fe2 <- cbind(rep(mean(fe.fe2_b), sims), simparam_fe2[,1:ncol(simparam_fe2)-1])

# Interpret RE M1
simparam_re1 <- MASS::mvrnorm(sims,pe.re1_b,vc.re1_b)
# Pull off the simulated lag coefficient
simphi_re1 <- simparam_re1[,ncol(simparam_re1)]
# Put together the "constant" term
simbetas_re1 <- simparam_re1[,1:ncol(simparam_re1)-1]

# Interpret RE M2
simparam_re2 <- MASS::mvrnorm(sims,pe.re2_b,vc.re2_b)
# Pull off the simulated lag coefficient
simphi_re2 <- simparam_re2[,ncol(simparam_re2)]
# Put together the "constant" term
simbetas_re2 <- simparam_re2[,2:ncol(simparam_re2)-1]

# Formula for FE M1
formula_fe1 <- "DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails"  
formula_fe1 <- as.formula(formula_fe1)

# Formula for FE M2
formula_fe2 <- "DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year + YearXMidwest + YearXWest + YearXSouth"  
formula_fe2 <- as.formula(formula_fe2)

#Formula for RE M1
formula_re1 <- "DemHouseShare ~ 1 + PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails"
formula_re1 <- as.formula(formula_re1)

#Formula for RE M2
formula_re2 <- "DemHouseShare ~ 1 + PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year + Midwest + West + South + YearXMidwest + YearXWest + YearXSouth"
formula_re2 <- as.formula(formula_re2)

#setup simcf xhyp objects
periods.out <- 2
xhyp_fe1 <- cfMake(formula_fe1, house_p, periods.out)
xhyp_fe2 <- cfMake(formula_fe2, house_p, periods.out)
xhyp_re1 <- cfMake(formula_re1, house_p, periods.out)
xhyp_re2 <- cfMake(formula_re2, house_p, periods.out)

#for each of the two forecast periods:
for(i in 1:periods.out){
  #FE M1
  xhyp_fe1 <- cfChange(xhyp_fe1, "Year", x=2019, xpre = 2017, scen=1)
  xhyp_fe1 <- cfChange(xhyp_fe1, "Year", x=2021, xpre = 2019, scen=2)

  xhyp_fe1 <- cfChange(xhyp_fe1, "PartisanMidterm", x = -1, xpre = 0, scen=1)
  xhyp_fe1 <- cfChange(xhyp_fe1, "PartisanMidterm", x = 0, xpre = -1, scen=2)
  
  xhyp_fe1 <- cfChange(xhyp_fe1, "PresUnem", x=-1*(3.6-5.97), scen=i)
  xhyp_fe1 <- cfChange(xhyp_fe1, "GovUnem", x=-1*(3.6-5.97), scen=1)
  xhyp_fe1 <- cfChange(xhyp_fe1, "GovUnem", x=1*(3.6-5.97), scen=2)

  xhyp_fe1 <- cfChange(xhyp_fe1, "GovCoattails", x=1, scen=1)
  xhyp_fe1 <- cfChange(xhyp_fe1, "PresCoattails", x=1, scen=2)
  
  #FE M2
  xhyp_fe2 <- cfChange(xhyp_fe2, "Year", x=2019, xpre = 2017, scen=1)
  xhyp_fe2 <- cfChange(xhyp_fe2, "Year", x=2021, xpre = 2019, scen=2)

  xhyp_fe2 <- cfChange(xhyp_fe2, "PartisanMidterm", x = -1, xpre = 0, scen=1)
  xhyp_fe2 <- cfChange(xhyp_fe2, "PartisanMidterm", x = 0, xpre = -1, scen=2)
  
  xhyp_fe2 <- cfChange(xhyp_fe2, "PresUnem", x=-1*(3.6-5.97), scen=i)
  xhyp_fe2 <- cfChange(xhyp_fe2, "GovUnem", x=-1*(3.6-5.97), scen=1)
  xhyp_fe2 <- cfChange(xhyp_fe2, "GovUnem", x=1*(3.6-5.97), scen=2)

  xhyp_fe2 <- cfChange(xhyp_fe2, "GovCoattails", x=1, scen=1)
  xhyp_fe2 <- cfChange(xhyp_fe2, "PresCoattails", x=1, scen=2)
  
  #RE M1
  xhyp_re1 <- cfChange(xhyp_re1, "Year", x=2019, xpre = 2017, scen=1)
  xhyp_re1 <- cfChange(xhyp_re1, "Year", x=2021, xpre = 2019, scen=2)
  
  xhyp_re1 <- cfChange(xhyp_re1, "PartisanMidterm", x = -1, xpre = 0, scen=1)
  xhyp_re1 <- cfChange(xhyp_re1, "PartisanMidterm", x = 0, xpre = -1, scen=2)
  
  xhyp_re1 <- cfChange(xhyp_re1, "PresUnem", x=-1*(3.6-5.97), scen=i)
  xhyp_re1 <- cfChange(xhyp_re1, "GovUnem", x=-1*(3.6-5.97), scen=1)
  xhyp_re1 <- cfChange(xhyp_re1, "GovUnem", x=1*(3.6-5.97), scen=2)

  xhyp_re1 <- cfChange(xhyp_re1, "GovCoattails", x=1, scen=1)
  xhyp_re1 <- cfChange(xhyp_re1, "PresCoattails", x=1, scen=2)
  
  #RE M2
  xhyp_re2 <- cfChange(xhyp_re2, "Year", x=2019, xpre = 2017, scen=1)
  xhyp_re2 <- cfChange(xhyp_re2, "Year", x=2021, xpre = 2019, scen=2)
  
  xhyp_re2 <- cfChange(xhyp_re2, "PartisanMidterm", x = -1, xpre = 0, scen=1)
  xhyp_re2 <- cfChange(xhyp_re2, "PartisanMidterm", x = 0, xpre = -1, scen=2)
  
  xhyp_re2 <- cfChange(xhyp_re2, "PresUnem", x=-1*(3.6-5.97), scen=i)
  xhyp_re2 <- cfChange(xhyp_re2, "GovUnem", x=-1*(3.6-5.97), scen=1)
  xhyp_re2 <- cfChange(xhyp_re2, "GovUnem", x=1*(3.6-5.97), scen=2)

  xhyp_re2 <- cfChange(xhyp_re2, "GovCoattails", x=1, scen=1)
  xhyp_re2 <- cfChange(xhyp_re2, "PresCoattails", x=1, scen=2)
} 

xhyp_fe1 #to show the counterfactual values (same in other 3 objects)

#compute average phi for model spec
phi_fe1 <- mean(simphi_fe1) 
phi_fe2 <- mean(simphi_fe2) 
phi_re1 <- mean(simphi_re1) 
phi_re2 <- mean(simphi_re2) 

#compute mean of Y for 2017
lagY <- mean(house_p$DemHouseShareL1[house_p$Year == 2017])

#simulate expected values for each model spec
sim.fe1 <- ldvsimev(xhyp_fe1,               # The matrix of hypothetical x's
                    simbetas_fe1,           # The matrix of simulated betas
                    ci=0.95,            # Desired confidence interval
                    phi=phi_fe1,            # estimated AR parameters; length must match lagY 
                    lagY=lagY)  # lags of y, most recent last

sim.fe2 <- ldvsimev(xhyp_fe2,               # The matrix of hypothetical x's
                    simbetas_fe2,           # The matrix of simulated betas
                    ci=0.95,            # Desired confidence interval
                    phi=phi_fe2,            # estimated AR parameters; length must match lagY 
                    lagY=lagY)  # lags of y, most recent last

sim.re1 <- ldvsimev(xhyp_re1,               # The matrix of hypothetical x's
                    simbetas_re1,           # The matrix of simulated betas
                    ci=0.95,            # Desired confidence interval
                    phi=phi_re1,            # estimated AR parameters; length must match lagY 
                    lagY=lagY)  # lags of y, most recent last

sim.re2 <- ldvsimev(xhyp_re2,               # The matrix of hypothetical x's
                    simbetas_re2,           # The matrix of simulated betas
                    ci=0.95,            # Desired confidence interval
                    phi=phi_re2,            # estimated AR parameters; length must match lagY 
                    lagY=lagY)  # lags of y, most recent last

#compile results as tidy df
simmed <- data.frame("FEvsRE" = c(rep("Fixed", 4), rep("Random", 4)),
                     "Spec" = rep(c("M1", "M1", "M2", "M2"), 2),
                      "Year" = rep(c(2019, 2021), 4),
                     "DemHouseShare" = c(sim.fe1$pe, sim.fe2$pe, sim.re1$pe, sim.re2$pe),
                     "lower" = c(sim.fe1$lower, sim.fe2$lower, sim.re1$lower, sim.re2$lower),
                     "upper" = c(sim.fe1$upper, sim.fe2$upper, sim.re1$lower, sim.re2$upper),
                     "se" = c(sim.fe1$se, sim.fe2$se, sim.re1$se, sim.re2$se))

#compute average state's ts
house_avg <- house_p %>%
  filter(Year >= 2000) %>%
  group_by(Year) %>% 
  summarize(DemHouseShare = mean(DemHouseShare))

ggplot(simmed, aes(x = Year, y = DemHouseShare, ymax = upper, ymin = lower)) +
  facet_wrap(Spec ~ FEvsRE, scales = "free_y") +
  geom_point(color = "blue") +
  geom_line(color = "blue") +
  geom_ribbon(alpha = .5, color = NA, fill = "blue") +
  geom_line(data=house_avg, aes(ymin = NULL, ymax = NULL)) +
  geom_abline(intercept = .50, slope = 0, linetype = 2, alpha = .5) +
  scale_y_continuous(labels = scales::percent)
```


### Part h

```{r prob 1.h}
#Using everything you have learned in this assignment and in the
#course, which of the four best models should we use to write-up our results,
#and why? (You may argue for multiple models if you think that’s appropriate.)
#What are your final substantive conclusions? Substantively, does it make much
#difference which model we choose? How does this affect the way you would
#write this analysis up in a paper?
```













