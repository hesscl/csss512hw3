---
title: "CSSS 512 HW 3"
author: "Chris Hess"
date: "May 10, 2018"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(forecast)
library(plm)
library(nlme)
library(simcf)

#key theoretical perspectives:

#1. economic performance
#2. spillovers/coattails
#3. trends

#M1 - control for midterm effects, presidential and gubernatorial unemployment 
#     effects, and presidential and gubernatorial coattails

#M2 - control for all M1 variables and region specific trends
```

## Problem 1

### Part a

```{r prob 1a}
#process data
house <- read_csv("statehouse.csv") %>%
  filter(GovCycle == 1, #gov cycle needs to be 2010, 2014, 2018...
         HouseTerm == 2) %>% #2 yr house term only
  arrange(State, Year) %>% #arrange by State, then year within States
  group_by(State) %>% #group rows by State for lagging
  mutate(DemHouseShareL1 = lag(DemHouseShare, 1), #use dplyr::lag with properly ordered tbl
         DemHouseShareL2 = lag(DemHouseShare, 2),
         DemHouseShareL3 = lag(DemHouseShare, 3),
         DemHouseShareL4 = lag(DemHouseShare, 4))

#double check work using dplyr::lag()
#house %>% 
#  select(State, Year, DemHouseShare, DemHouseShareL1, DemHouseShareL2, DemHouseShareL3, DemHouseShareL4) %>% 
#  View()

#28 states
length(unique(house$State))

#20 periods
length(unique(house$Year))
```

### Part b

Looks like AR(1) processes in most states. Some of the observed time-series look non-stationary given no reversion to a mean over the full length of periods. The nonstationary conclusion is also supported by a fair number `adf.test` and `PP.test` p-values falling outside of the conventional p<.05 threshold. States with clear change over the period show decreashing democratic vote shares over the full set of periods, though there are states like New York which oppose this direction. The IPS tests suggest that we could consider the panel as stationary with individual intercepts and trends.

```{r prob 1b}
#make some file structure for output
if(!dir.exists("output")){
  dir.create("output")
  dir.create("output/ts")
  dir.create("output/acf")
  dir.create("output/pacf")
}

#empty obj for unit root test results
unitroot.tbl <- NULL

#iterate through states
for(state in unique(house$State)){
  
  #pull the state's ts
  ts <- house %>%
    filter(State == state) %>%
    pull(DemHouseShare)
  
  #make it ts class
  ts <- ts(ts, start = 1978, end = 2016, frequency = 1/2)
  
  #plot the ts
  TS <- autoplot(ts) +
    scale_y_continuous(labels = scales::percent) +
    labs(title = paste0("Democratic House Share for ", house$Statename[house$State == state])) +
    theme_minimal()
  
  #plot the ACF
  ACF <- ggAcf(ts) +
    labs(title = paste0("ACF for ", house$Statename[house$State == state])) +
    theme_minimal()
  
  #plot the PACF
  PACF <- ggPacf(ts) +
    labs(title = paste0("PACF for ", house$Statename[house$State == state])) +
    theme_minimal()
  
  #rum ADF test, parse results to row
  ADF <- tseries::adf.test(ts)
  ADF.row <- data.frame(state = state, 
                        test = "ADF", 
                        statistic = ADF[['statistic']], 
                        parameter = ADF[['parameter']], 
                        p.value = ADF[['p.value']],
                        stringsAsFactors = F)
  
  #run PP test, parse results to row
  PP <- PP.test(ts)
  PP.row <- data.frame(state = state, 
                       test = "PP", 
                       statistic = PP[['statistic']], 
                       parameter = PP[['parameter']],
                       p.value = PP[['p.value']],
                       stringsAsFactors = F)

  #write graphics to pdf
  ggsave(TS, filename = paste0("./output/ts/", state, "_ts.pdf"),
         width = 8, height = 6, dpi = 300)
  ggsave(ACF, filename = paste0("./output/acf/", state, "_acf.pdf"),
         width = 8, height = 6, dpi = 300)
  ggsave(PACF, filename = paste0("./output/pacf/", state, "_pacf.pdf"),
         width = 8, height = 6, dpi = 300)
  
  #add row to unit root tbl
  unitroot.tbl <- bind_rows(unitroot.tbl, ADF.row, PP.row)
}

#panel unit root test
ts <- with(house, data.frame(split(DemHouseShare, as.character(State))))
purtest(ts, pmax = 4, exo = "intercept", house_p, test = "ips")
purtest(ts, pmax = 4, exo = "trend", house_p, test = "ips")

#write unit root tbl to storage
write_csv(unitroot.tbl, "output/unitroot_tests.csv")

#plot unit root p.val dist
ggplot(unitroot.tbl, aes(x = p.value, fill = test)) + 
  facet_wrap(~ test) +
  geom_histogram(binwidth = .05) +
  theme_minimal() +
  scale_fill_brewer(palette = "Set1")
```

### Part c

```{r prob 1.c}
house <- groupedData(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails | State, data = house)

#no dynamics, just covariates + state random effect
re_m1_a <- lme(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails,
            random = ~ 1,
            data = house)
summary(re_m1_a)

#AR(1)
re_m1_b <- lme(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails,
            random = ~ 1,
            correlation = corARMA(p = 1, q = 0),
            data = house)
summary(re_m1_b)

#MA(1)
re_m1_c <- lme(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails,
            random = ~ 1,
            correlation = corARMA(p = 0, q = 1),
            data = house)
summary(re_m1_c)

#ARMA(1,1)
re_m1_d <- lme(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails,
            random = ~ 1,
            correlation = corARMA(p = 1, q = 1),
            data = house)
summary(re_m1_d)

#AR(1) appears to fit best. The MA(1) contribution is small, and higher-order 
#autoregression also does not seem to be present
```

 
### Part d 
 
```{r prob 1.d}
#M1 + region specific trends
house <- groupedData(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year + (Year * Midwest) + (Year * South) + (Year * West) | State, data = house)

#no dynamics, just covariates + state random effect
re_m2_a <- lme(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year * Midwest + Year * South + Year * West,
            random = ~ 1,
            data = house)
summary(re_m2_a)

#AR(1)
re_m2_b <- lme(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year * Midwest + Year * South + Year * West,
            random = ~ 1,
            correlation = corARMA(p = 1, q = 0),
            data = house)
summary(re_m2_b)

#MA(1)
re_m2_c <- lme(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year * Midwest + Year * South + Year * West,
            random = ~ 1,
            correlation = corARMA(p = 0, q = 1),
            data = house)
summary(re_m2_c)

#ARMA(1,1)
re_m2_d <- lme(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year * Midwest + Year * South + Year * West,
            random = ~ 1,
            correlation = corARMA(p = 1, q = 1),
            data = house)
summary(re_m2_d)

#model b (AR(1)) is best
```

 
### Part e

```{r prob 1.e}
house_p <- pdata.frame(house, index = "State")

fe_m1_a <- plm(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails,  data= house_p, model = "within", effect = "individual")
summary(fe_m1_a)

fe_m1_b <- plm(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + DemHouseShareL1, data= house_p, model = "within", effect = "individual")
summary(fe_m1_b)

fe_m1_c <- plm(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + DemHouseShareL1 + DemHouseShareL2, data= house_p, model = "within", effect = "individual")
summary(fe_m1_c)

fe_m1_d <- plm(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + DemHouseShareL1 + DemHouseShareL2 + DemHouseShareL3, data= house_p, model = "within", effect = "individual")
summary(fe_m1_d)

#model b (AR(1)) is best
```

### Part f

```{r prob 1.f}
#M1 + region specific trends
fe_m2_a <- plm(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year * South + Year * Midwest + Year * West, data= house_p, model = "within", effect = "individual")
summary(fe_m2_a)

fe_m2_b <- plm(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year * South + Year * Midwest + Year * West + DemHouseShareL1, data= house_p, model = "within", effect = "individual")
summary(fe_m2_b)

fe_m2_c <- plm(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year * South + Year * Midwest + Year * West + DemHouseShareL1 + DemHouseShareL2, data= house_p, model = "within", effect = "individual")
summary(fe_m2_c)

fe_m2_d <- plm(DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year * South + Year * Midwest + Year * West + DemHouseShareL1 + DemHouseShareL2 + DemHouseShareL3, data= house_p, model = "within", effect = "individual")
summary(fe_m2_d)

#model b (AR(1)) is best
```

### Part g

```{r prob 1.g}
#Using each of four “best” models, forecast what will happen to
#the size of the Democratic majority in the average state in the 2019 and 2021
#sessions for the following single scenario. Assume the Democrats resume this
#state’s governorship in 2019 and the presidency in 2021, and compute appropriate
#counterfactual values of PartisanMidterm, PresCoattails, GovCoattails. Assume
#unemployment falls to 3.6% for both elections and construct PresUnem and
#GovUnem accordingly. Set all trend variables at the average value they will take
#across regions in 2019 and 2021, respectively. Make appropriate assumptions for
#the prior value(s) of the outcome variable (e.g., the average Democratic House
#share in 2017).

#For each model, report or graph the predicted Democratic majority and its 95%
#confidence (or predictive) interval for the 2019 and 2021 sessions. Describe the
#substantive impact of your forecast results in as much detail as you feel comfortable,
#as well as how much confidence we should have in the forecasts. Be sure to
#consider the scale of the outcome variable in assessing what counts as a substantively
#large or small change.

#NB: As a check on your work, report the table of counterfactual covariate values
#you used to make your forecasts. Be very careful when constructing these values
#to capture to logic of the covariates; each one is tricky in its own way. To carry
#out the forecasts, use the simcf library’s ldvsimev(), pay close attention to the
#example code, and think through all modifications you need to make.

house <- house %>%
  select(State, DemHouseShare, PartisanMidterm, PresUnem, GovUnem, PresCoattails,
         GovCoattails, Year, West, South, Midwest, DemHouseShareL1) %>%
  na.omit() %>%
  arrange(State, Year) %>%
  group_by(State) %>%
  mutate(YearXMidwest = Year* Midwest,
         YearXWest = Year * West,
         YearXSouth = Year * South)
house_p <- pdata.frame(house, index = "State")

fe_m2_b <- plm(DemHouseShare ~ 1 + PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year + YearXMidwest + YearXWest + YearXSouth + DemHouseShareL1, data= house_p, model = "within", effect = "individual")
re_m2_b <- plm(DemHouseShare ~ 1 + PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year + Year * Midwest + Year * West + Year * South + DemHouseShareL1, data= house_p, model = "random", effect = "individual")
phtest(fe_m2_b, re_m2_b) #suggests FE specification is necessary

# Extract model results
pe.fe2_b <- coef(fe_m2_b)                                 # Point estimates of parameters
vc.fe2_b <- vcov(fe_m2_b)                                   # Var-cov matrix of point estimates
se.fe2_b <- sqrt(diag(vc.fe2_b))                            # std erros of point estimates
tstat.fe2_b <- abs(pe.fe2_b/se.fe2_b)                        # t-statistics
df.fe2_b <- rep(fe_m1_b$df.residual, length(tstat.fe2_b))  # residual degrees of freedom
pval.fe2_b <- 2*pt(tstat.fe2_b, df.fe2_b, lower.tail=FALSE)  # p-values
fe.fe2_b <- fixef(fe_m2_b)                                # the (removed) fixed effects by group 
resid.fe2_b <- resid(fe_m2_b)                             # Residuals

# Interpret the model using custom code
sims <- 10000
simparam <- MASS::mvrnorm(sims,pe.fe2_b,vc.fe2_b)
# Pull off the simulated lag coefficient
simphi <- simparam[,ncol(simparam)]
# Put together the "constant" term (avg of the FEs, or a specific FE if you like)
# with the rest of the regressors
simbetas <- cbind(rep(mean(fe.fe2_b), sims), simparam[,1:ncol(simparam)-1])

formula <- "DemHouseShare ~ PartisanMidterm + PresUnem + GovUnem + PresCoattails + GovCoattails + Year + YearXMidwest + YearXWest + YearXSouth"  
formula <- as.formula(formula)
  
periods.out <- 2
xhyp <- cfMake(formula, house_p, periods.out)
for(i in 1:periods.out){
  xhyp <- cfChange(xhyp, "Year", x=2019, xpre = 2017, scen=1)
  xhyp <- cfChange(xhyp, "Year", x=2021, xpre = 2019, scen=2)

  xhyp <- cfChange(xhyp, "PartisanMidterm", x = -1, xpre = 0, scen=1)
  xhyp <- cfChange(xhyp, "PartisanMidterm", x = 0, xpre = -1, scen=2)
  
  xhyp <- cfChange(xhyp, "PresUnem", x=-1*(3.6-5.97), scen=i)
  xhyp <- cfChange(xhyp, "GovUnem", x=-1*(3.6-5.97), scen=1)
  xhyp <- cfChange(xhyp, "GovUnem", x=1*(3.6-5.97), scen=2)

  xhyp <- cfChange(xhyp, "GovCoattails", x=1, scen=1)
  xhyp <- cfChange(xhyp, "PresCoattails", x=1, scen=2)
}  

phi <- mean(simphi) 
lagY <- mean(house_p$DemHouseShareL1[house_p$Year == 2017])

# Simulate expected values of Y (on original level scale)
# out to periods.out given hypothetical future values of X,
# initial lags of the change in Y, and an initial level of Y
sim.ev2 <- ldvsimev(xhyp,               # The matrix of hypothetical x's
                    simbetas,           # The matrix of simulated betas
                    ci=0.95,            # Desired confidence interval
                    phi=phi,            # estimated AR parameters; length must match lagY 
                    lagY=lagY)  # lags of y, most recent last

sim.ev2

simmed <- data.frame("Year" = c(2019, 2021),
                     "DemHouseShare" = sim.ev2$pe,
                     "lower" = sim.ev2$lower,
                     "upper" = sim.ev2$upper,
                     "se" = sim.ev2$se)
house_avg <- house_p %>%
  group_by(Year) %>% 
  summarize(DemHouseShare = mean(DemHouseShare))

ggplot(simmed, aes(x = Year, y = DemHouseShare, ymax = upper, ymin = lower)) +
  geom_point(color = "blue") +
  geom_line(color = "blue") +
  geom_ribbon(alpha = .5, color = NA, fill = "blue") +
  geom_line(data=house_avg, aes(ymin = NULL, ymax = NULL)) +
  geom_abline(intercept = .50, slope = 0, linetype = 2, alpha = .5) +
  scale_y_continuous(labels = scales::percent)
```


### Part h

```{r prob 1.h}
#Using everything you have learned in this assignment and in the
#course, which of the four best models should we use to write-up our results,
#and why? (You may argue for multiple models if you think that’s appropriate.)
#What are your final substantive conclusions? Substantively, does it make much
#difference which model we choose? How does this affect the way you would
#write this analysis up in a paper?
```













